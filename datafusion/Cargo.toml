# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

[package]
name = "datafusion"
description = "DataFusion is an in-memory query engine that uses Apache Arrow as the memory model"
version = "6.0.0"
homepage = "https://github.com/apache/arrow-datafusion"
repository = "https://github.com/apache/arrow-datafusion"
readme = "../README.md"
authors = ["Apache Arrow <dev@arrow.apache.org>"]
license = "Apache-2.0"
keywords = [ "arrow", "query", "sql" ]
include = [
    "benches/*.rs",
    "src/**/*.rs",
    "Cargo.toml",
]
edition = "2021"
rust-version = "1.57"

[lib]
name = "datafusion"
path = "src/lib.rs"

[features]
default = ["crypto_expressions", "regex_expressions", "unicode_expressions"]
simd = ["arrow/simd"]
<<<<<<< HEAD
crypto_expressions = ["md-5", "sha2", "blake2", "blake3"]
=======
crypto_expressions = ["md-5", "sha2"]
>>>>>>> ExternalSortExec v1
regex_expressions = ["regex"]
unicode_expressions = ["unicode-segmentation"]
# FIXME: add pyarrow support to arrow2 pyarrow = ["pyo3", "arrow/pyarrow"]
pyarrow = ["pyo3"]
# Used for testing ONLY: causes all values to hash to the same value (test for collisions)
force_hash_collisions = []
# Used to enable the avro format
avro = ["avro-rs", "num-traits"]

[dependencies]
ahash = "0.7"
hashbrown = { version = "0.11", features = ["raw"] }
parquet = { package = "parquet2", version = "0.8", default_features = false, features = ["stream"] }
sqlparser = "0.13"
paste = "^1.0"
num_cpus = "1.13.0"
chrono = "0.4"
async-trait = "0.1.41"
futures = "0.3"
pin-project-lite= "^0.2.7"
tokio = { version = "1.0", features = ["macros", "rt", "rt-multi-thread", "sync", "fs"] }
tokio-stream = "0.1"
log = "^0.4"
md-5 = { version = "^0.9.1", optional = true }
sha2 = { version = "^0.9.1", optional = true }
blake2 = { version = "^0.9.2", optional = true }
blake3 = { version = "1.0", optional = true }
ordered-float = "2.0"
unicode-segmentation = { version = "^1.7.1", optional = true }
regex = { version = "^1.4.3", optional = true }
lazy_static = { version = "^1.4.0" }
smallvec = { version = "1.6", features = ["union"] }
rand = "0.8"
avro-rs = { version = "0.13", features = ["snappy"], optional = true }
num-traits = { version = "0.2", optional = true }
pyo3 = { version = "0.14", optional = true }
uuid = { version = "0.8", features = ["v4"] }
tempfile = "3"

[dependencies.arrow]
package = "arrow2"
version="0.8"
features = ["io_csv", "io_json", "io_parquet", "io_parquet_compression", "io_ipc", "io_print", "ahash",
    "compute_merge_sort", "compute_concatenate", "compute_regex_match", "compute_arithmetics",
    "compute_cast", "compute_partition", "compute_temporal", "compute_take", "compute_aggregate",
    "compute_comparison", "compute_if_then_else", "compute_nullif", "compute_boolean", "compute_length",
    "compute_limit", "compute_boolean_kleene", "compute_like", "compute_filter", "compute_window",]

[dev-dependencies]
criterion = "0.3"
doc-comment = "0.3"
parquet-format-async-temp = "0"

[[bench]]
name = "aggregate_query_sql"
harness = false

[[bench]]
name = "sort_limit_query_sql"
harness = false

[[bench]]
name = "math_query_sql"
harness = false

[[bench]]
name = "filter_query_sql"
harness = false

[[bench]]
name = "window_query_sql"
harness = false

[[bench]]
name = "scalar"
harness = false

[[bench]]
name = "physical_plan"
harness = false
